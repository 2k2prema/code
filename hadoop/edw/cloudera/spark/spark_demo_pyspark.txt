#Introduction
#spark-shell
#pyspark
#SparkContext
#SQLContext
#HiveContext
#spark-sql (only latest version)
#JDBC
#To connect to remote database using jdbc
#Either you need to run pyspark with driver-class-path or set environment variable with os.environ
pyspark --driver-class-path /usr/share/java/mysql-connector-java.jar
os.environ['SPARK_CLASSPATH'] = "/usr/share/java/mysql-connector-java.jar"

from pyspark.sql import SQLContext

sqlContext = SQLContext(sc)
jdbcurl = "jdbc:mysql://quickstart.cloudera:3306/retail_db?user=retail_dba&password=cloudera"
df = sqlContext.load(source="jdbc", url=jdbcurl, dbtable="departments")

for rec in df.collect():
  print(rec)

df.count()

##############################################################################

#Developing simple scala based applications for spark
#Save this to a file with py extension
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)
dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
for line in dataRDD.collect():
    print(line)
dataRDD.saveAsTextFile("/user/cloudera/pyspark/departmentsTesting")

#Run using this command
#master local will run in spark native mode
spark-submit --master local saveFile.py

#master yarn will run in yarn mode
spark-submit --master yarn saveFile.py

##############################################################################

# Load data from HDFS and storing results back to HDFS using Spark
from pyspark import SparkContext

dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
for line in dataRDD.collect():
    print(line)

print(dataRDD.count())

dataRDD.saveAsTextFile("/user/cloudera/pyspark/departments")

#Object files are not available in python
dataRDD.saveAsObjectFile("/user/cloudera/pyspark/departmentsObject")

#saveAsSequenceFile
dataRDD.map(lambda x: (None, x)).saveAsSequenceFile("/user/cloudera/pyspark/departmentsSeq")
dataRDD.map(lambda x: tuple(x.split(",", 1))).saveAsSequenceFile("/user/cloudera/pyspark/departmentsSeq")
dataRDD.map(lambda x: tuple(x.split(",", 1))).saveAsSequenceFile("/user/cloudera/pyspark/orders")
path="/user/cloudera/pyspark/departmentsSeq"

dataRDD.map(lambda x: tuple(x.split(",", 1))).saveAsNewAPIHadoopFile(path,"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat",keyClass="org.apache.hadoop.io.Text",valueClass="org.apache.hadoop.io.Text")

#reading sequence file
data = sc.sequenceFile("/user/cloudera/pyspark/departmentsSeq")
data = sc.sequenceFile("/user/cloudera/pyspark/orders")
data = sc.sequenceFile("/user/cloudera/pyspark/departmentsSeq", "org.apache.hadoop.io.IntWritable", "org.apache.hadoop.io.Text")
for rec in data.collect():
  print(rec)

from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
depts = sqlContext.sql("select * from departments")
for rec in depts.collect():
  print(rec)

sqlContext.sql("create table departmentsSpark as select * from departments")
depts = sqlContext.sql("select * from departmentsSpark")
for rec in depts.collect():
  print(rec)

#We can run hive INSERT, LOAD and any valid hive query in Hive context

#Make sure you copy departments.json to HDFS
#create departments.json on Linux file system
{"department_id":2, "department_name":"Fitness"}
{"department_id":3, "department_name":"Footwear"}
{"department_id":4, "department_name":"Apparel"}
{"department_id":5, "department_name":"Golf"}
{"department_id":6, "department_name":"Outdoors"}
{"department_id":7, "department_name":"Fan Shop"}
{"department_id":8, "department_name":"TESTING"}
{"department_id":8000, "department_name":"TESTING"}

#copying to HDFS (using linux command line)
hadoop fs -put departments.json /user/cloudera/pyspark

from pyspark import SQLContext
sqlContext = SQLContext(sc)
departmentsJson = sqlContext.jsonFile("/user/cloudera/pyspark/departments.json")
departmentsJson.registerTempTable("departmentsTable")
departmentsData = sqlContext.sql("select * from departmentsTable")
for rec in departmentsData.collect():
  print(rec)

#Writing data in json format
departmentsData.toJSON().saveAsTextFile("/user/cloudera/pyspark/departmentsJson")

#Validating the data
hadoop fs -cat /user/cloudera/pyspark/departmentsJson/part*

##############################################################################

# Join disparate datasets together using Spark
# Problem statement, get the revenue and number of orders on daily basis
ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

ordersParsedRDD = ordersRDD.map(lambda rec: (rec.split(",")[0], rec))
orderItemsParsedRDD = orderItemsRDD.map(lambda rec: (rec.split(",")[1], rec))

ordersJoinOrderItems = ordersParsedRDD.join(orderItemsParsedRDD)
joinData = ordersJoinOrderItems.map(lambda t: (t[1][0].split(",")[1], float(t[1][1].split(",")[4])))

ordersAggRDD = joinData.aggregateByKey( \
(0, 0), \
lambda subtotal1, subtotal2: (subtotal1[0] + subtotal2, subtotal1[1] + 1), \
lambda total1, total2: (round(total1[0] + total2[0], 2), total1[1] + total2[1]) \
)

for data in ordersAggRDD.collect():
  print(data)

##############################################################################

# Calculate aggregate statistics (e.g., average or sum) using Spark
#sum
#Number of orders by status
ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
ordersMap = ordersRDD.map(lambda rec:  (rec.split(",")[3], 1))
ordersByStatus = ordersMap.reduceByKey(lambda x,y: x + y)

for recs in ordersByStatus.collect():
  print(recs)

for recs in ordersRDD.map(lambda rec:  (rec.split(",")[3], 1)).reduceByKey(lambda x,y: x + y).collect():
  print(recs)

#Number of orders by order date and order status
#Key orderDate and orderStatus
ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
ordersMapRDD = ordersRDD.map(lambda rec: ((rec.split(",")[1], rec.split(",")[3]), 1))
ordersMapRDD.reduceByKey(lambda v1, v2: v1+v2).sortByKey().collect().foreach(println)

#average
#average revenue per order per day
ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

ordersParsedRDD = ordersRDD.map(lambda rec: (rec.split(",")[0], rec))
orderItemsParsedRDD = orderItemsRDD.map(lambdarec: (rec.split(",")[1], rec))

ordersJoinOrderItems = ordersParsedRDD.join(orderItemsParsedRDD)
joinData = ordersJoinOrderItems.map(lambda t: (t[1][0].split(",")[1], float(t[1][1].split(",")[4])))

ordersAggRDD = joinData.reduceByKey( \
lambda total1, total2: total1 + total2 \
)

for data in ordersAggRDD.collect():
  print(data)

ordersPerDayRDD = ordersRDD.
    map(lambda rec: (rec.split(",")[1], 1)).
    reduceByKey(lambda subtotal1, subtotal2: subtotal1+subtotal2)
for data in ordersPerDayRDD.collect():
  print(data)

ordersAvgJoinData = ordersAggRDD.join(ordersPerDayRDD)
for data in ordersAvgJoinData.map(t => (t[0], t[1][0]/t[1][1])).sortByKey().collect():
  print data

##########################################################################################

# Using data frames (only works with spark 1.3.0 or later)
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
import sqlContext.implicits._
sqlContext.sql("set spark.sql.shuffle.partitions=10");

case class Orders(orderId: Int, orderDate: String, orderCustomerId: Int, orderStatus: String)

val orders = sc.textFile("/user/cloudera/sqoop_import/orders").
  map(rec => rec.split(",")).
  map(o => Orders(o(0).toInt, o(1), o(2).toInt, o(3))).toDF()

orders.registerTempTable("orders")
val ordersData = sqlContext.sql("select * from orders")
ordersData.collect().take(10).foreach(println)

case class OrderItems
  (orderItemId: Int, 
   orderItemOrderId: Int, 
   orderItemProductId: Int, 
   orderItemQuantity: Int, 
   orderItemSubtotal: Float, 
   orderItemProductPrice: Float
  )

val orderItems = sc.textFile("/user/cloudera/sqoop_import/order_items").
  map(rec => rec.split(",")).
  map(oi => OrderItems(oi(0).toInt, oi(1).toInt, oi(2).toInt, oi(3).toInt, oi(4).toFloat, oi(5).toFloat)).
  toDF()

orderItems.registerTempTable("orderItems")
val orderItemsData = sqlContext.sql("select * from orderItems")
orderItemsData.collect().take(10).foreach(println)

val ordersJoinOrderItems = sqlContext.sql("select o.orderDate, sum(oi.orderItemSubtotal), count(o.orderId) from orders o join orderItems oi on o.orderId = oi.orderItemOrderId group by o.orderDate")
val ordersJoinOrderItems = sqlContext.sql("select o.orderDate, sum(oi.orderItemSubtotal), count(distinct o.orderId), sum(oi.orderItemSubtotal)/count(distinct o.orderId) from orders o join orderItems oi on o.orderId = oi.orderItemOrderId group by o.orderDate order by o.orderDate")
ordersJoinOrderItems.collect().take(10).foreach(println)

##########################################################

# Filter data into a smaller dataset using Spark
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
ordersRDD.filter(line => line.split(",")(3).equals("COMPLETE")).take(5).foreach(println)
ordersRDD.filter(line => line.split(",")(3).contains("PENDING")).take(5).foreach(println)
ordersRDD.filter(line => line.split(",")(0).toInt > 100).take(5).foreach(println)
ordersRDD.filter(line => line.split(",")(0).toInt > 100 || line.split(",")(3).contains("PENDING")).take(5).foreach(println)
ordersRDD.filter(line => line.split(",")(0).toInt > 1000 && 
    (line.split(",")(3).contains("PENDING") || line.split(",")(3).equals("CANCELLED"))).
    take(5).
    foreach(println)
ordersRDD.filter(line => line.split(",")(0).toInt > 1000 && 
    !line.split(",")(3).equals("COMPLETE")).
    take(5).
    foreach(println)

# Write a query that produces ranked or sorted data using Spark
